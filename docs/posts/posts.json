[
  {
    "path": "posts/2022-02-21-frequency-counts-from-lists/",
    "title": "Frequency counts from lists",
    "description": "In this post I look at better ways of obtaining frequency distributions from a Python list, using methods **other than pandas**",
    "author": [
      {
        "name": "Abhijit Dasgupta",
        "url": {}
      }
    ],
    "date": "2022-02-21",
    "categories": [
      "Python"
    ],
    "contents": "\n\nContents\nIntroducing the problem\nBetter ways\nUsing collections\nUsing numpy\nTimings\n\n\nIntroducing the problem\nHow to you compute a frequency table from a list in Python? We’ll use\na list named dat which is created by sampling the first 5\nletters.\n\nimport string\nimport random\n\ndat = random.choices(string.ascii_lowercase[:5], k = 100)\n\nOne way, that I use all the time, is\n\nimport pandas as pd\npd.Series(dat).value_counts()\ne    25\nb    22\na    20\nc    19\nd    14\ndtype: int64\n\nBut can you do this without pandas? Turns out, you can\nBetter ways\nUsing collections\n\nimport collections\npd.Series(collections.Counter(dat))\ne    25\nb    22\nc    19\nd    14\na    20\ndtype: int64\n\nThis creates a Series out of the output of Counter, which is a\ndict-like object.\nUsing numpy\n\nimport numpy as np\nans = np.unique(dat, return_counts=True)\n\nThis returns a tuple, which can be converted several ways, per this StackOverflow\nanswer\n\npd.DataFrame(np.column_stack(ans), columns = ['item','count'])\n  item count\n0    a    20\n1    b    22\n2    c    19\n3    d    14\n4    e    25\npd.DataFrame(np.vstack(ans).T, columns = ['item','count'])\n  item count\n0    a    20\n1    b    22\n2    c    19\n3    d    14\n4    e    25\npd.DataFrame(np.transpose(ans), columns = ['item','count'])\n  item count\n0    a    20\n1    b    22\n2    c    19\n3    d    14\n4    e    25\n\nTimings\nLet’s look at timings for each of these methods.\n\nimport timeit\nmysetup = \"\"\"\nimport pandas as pd\nimport numpy as np\nimport collections, random, string\ndat = random.choices(string.ascii_lowercase[:5], k=1000)\ndat1 = pd.Series(dat)\n\"\"\"\n\n\ntimeit.timeit('pd.Series(dat).value_counts()', setup=mysetup, number=1000)/1000\n0.0005792085190000002\ntimeit.timeit('pd.Series(collections.Counter(dat))', setup=mysetup, number=1000)/1000\n0.00025398987399999974\ntimeit.timeit('pd.DataFrame(np.column_stack(np.unique(dat, return_counts=True)))', setup=mysetup, number=1000)/1000\n0.0002821108809999999\ntimeit.timeit('pd.DataFrame(np.vstack(np.unique(dat, return_counts=True)).T)', setup=mysetup, number=1000)/1000\n0.00027867445300000027\ntimeit.timeit('pd.DataFrame(np.transpose(np.unique(dat, return_counts=True)))', setup=mysetup, number=1000)/1000\n0.0002735296419999997\n\nSo we can see that my usual strategy is half as efficient as\nthe other strategies described in this post !!\nWhat about if we start with a pandas Series anyway, and forgo the\ncasting into a pandas structure?\n\ntimeit.timeit(\"dat1.value_counts()\", setup=mysetup, number=1000)/1000\n0.0003499007340000002\ntimeit.timeit(\"collections.Counter(dat1)\", setup=mysetup, number=1000)/1000\n0.00011676514499999957\n\nEven here, the pandas method is at quite the\ndisadvantage.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-22T02:09:40-05:00",
    "input_file": {}
  }
]
